{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Hyperparameter Tuning for Machine Learning Classifiers\n",
    "# This notebook demonstrates methods for optimizing classifier performance through hyperparameter tuning\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "```\n",
    "\n",
    "# Introduction to Hyperparameter Tuning\n",
    "\n",
    "Hyperparameters are model configuration settings used to control the learning process. Unlike model parameters that are learned during training, hyperparameters must be set before training begins. Proper tuning can significantly improve model performance.\n",
    "\n",
    "Key concepts:\n",
    "1. Cross-validation: Evaluating model performance on different data splits\n",
    "2. Grid Search: Systematic search through specified parameter values\n",
    "3. Random Search: Sampling from parameter distributions\n",
    "4. Validation curves: Visualizing model performance vs. hyperparameters\n",
    "\n",
    "```python\n",
    "# Generate sample dataset (similar to previous lesson)\n",
    "np.random.seed(42)\n",
    "n_samples = 300\n",
    "\n",
    "# Create more complex synthetic data with 3 features\n",
    "X1 = np.random.normal(loc=[8, 2, 3], scale=[1, 1, 1], size=(n_samples//2, 3))\n",
    "X2 = np.random.normal(loc=[2, 8, 6], scale=[1, 1, 1], size=(n_samples//2, 3))\n",
    "\n",
    "X = np.vstack([X1, X2])\n",
    "y = np.hstack([np.zeros(n_samples//2), np.ones(n_samples//2)])\n",
    "\n",
    "# Split and scale data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "# Cross-Validation\n",
    "\n",
    "Cross-validation helps assess model performance more robustly than a single train-test split.\n",
    "\n",
    "```python\n",
    "# Demonstrate cross-validation with KNN\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "cv_scores = cross_val_score(knn, X_train_scaled, y_train, cv=5)\n",
    "\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean CV score:\", cv_scores.mean())\n",
    "print(\"Standard deviation:\", cv_scores.std())\n",
    "```\n",
    "\n",
    "# Grid Search\n",
    "\n",
    "Grid Search exhaustively searches through a specified parameter grid to find the best combination.\n",
    "\n",
    "```python\n",
    "# Grid Search for KNN\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9, 11],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "grid_search_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid_knn,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_knn.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search_knn.best_score_)\n",
    "```\n",
    "\n",
    "# Random Search\n",
    "\n",
    "Random Search samples from parameter distributions, often finding good parameters more efficiently than Grid Search.\n",
    "\n",
    "```python\n",
    "# Random Search for SVM\n",
    "param_distributions = {\n",
    "    'C': np.logspace(-3, 3, 1000),\n",
    "    'kernel': ['rbf', 'linear'],\n",
    "    'gamma': np.logspace(-3, 3, 1000)\n",
    "}\n",
    "\n",
    "random_search_svm = RandomizedSearchCV(\n",
    "    SVC(),\n",
    "    param_distributions,\n",
    "    n_iter=100,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "random_search_svm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters:\", random_search_svm.best_params_)\n",
    "print(\"Best cross-validation score:\", random_search_svm.best_score_)\n",
    "```\n",
    "\n",
    "# Validation Curves\n",
    "\n",
    "Validation curves help visualize how model performance changes with hyperparameter values.\n",
    "\n",
    "```python\n",
    "def plot_validation_curve(param_name, param_range, model, X, y, cv=5):\n",
    "    train_scores, test_scores = np.zeros((cv, len(param_range))), np.zeros((cv, len(param_range)))\n",
    "    \n",
    "    for i, param_value in enumerate(param_range):\n",
    "        # Set parameter and perform cross-validation\n",
    "        model.set_params(**{param_name: param_value})\n",
    "        cv_scores = cross_val_score(model, X, y, cv=cv)\n",
    "        test_scores[:, i] = cv_scores\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(param_range, test_scores.mean(axis=0), label='Cross-validation score')\n",
    "    plt.fill_between(param_range, \n",
    "                     test_scores.mean(axis=0) - test_scores.std(axis=0),\n",
    "                     test_scores.mean(axis=0) + test_scores.std(axis=0), \n",
    "                     alpha=0.2)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Validation Curve for {param_name}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example: Validation curve for KNN n_neighbors\n",
    "n_neighbors_range = np.arange(1, 31, 2)\n",
    "plot_validation_curve('n_neighbors', n_neighbors_range, KNeighborsClassifier(), \n",
    "                     X_train_scaled, y_train)\n",
    "```\n",
    "\n",
    "# Random Forest Parameter Tuning\n",
    "\n",
    "Random Forests have several important hyperparameters to tune.\n",
    "\n",
    "```python\n",
    "# Grid Search for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search_rf = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search_rf.best_params_)\n",
    "print(\"Best cross-validation score:\", grid_search_rf.best_score_)\n",
    "```\n",
    "\n",
    "# Comparing Tuned Models\n",
    "\n",
    "```python\n",
    "# Compare base models vs tuned models\n",
    "models = {\n",
    "    'Base KNN': KNeighborsClassifier(),\n",
    "    'Tuned KNN': grid_search_knn.best_estimator_,\n",
    "    'Base SVM': SVC(),\n",
    "    'Tuned SVM': random_search_svm.best_estimator_,\n",
    "    'Base RF': RandomForestClassifier(),\n",
    "    'Tuned RF': grid_search_rf.best_estimator_\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    train_score = model.score(X_train_scaled, y_train)\n",
    "    test_score = model.score(X_test_scaled, y_test)\n",
    "    results[name] = {'Train Score': train_score, 'Test Score': test_score}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "# Tips for Hyperparameter Tuning\n",
    "\n",
    "1. Start with broad parameter ranges and refine\n",
    "2. Use RandomizedSearchCV for large parameter spaces\n",
    "3. Consider computational cost vs potential improvement\n",
    "4. Watch for overfitting (large gap between training and validation scores)\n",
    "5. Use domain knowledge to guide parameter selection\n",
    "\n",
    "```python\n",
    "# Example of parameter refinement\n",
    "# After finding approximate good values, we can search more finely around them\n",
    "refined_param_grid = {\n",
    "    'n_neighbors': [grid_search_knn.best_params_['n_neighbors'] - 1, grid_search_knn.best_params_['n_neighbors'], grid_search_knn.best_params_['n_neighbors'] + 1],\n",
    "    'weights': [grid_search_knn.best_params_['weights']],\n",
    "    'metric': [grid_search_knn.best_params_['metric']]\n",
    "}\n",
    "\n",
    "refined_grid_search_knn = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    refined_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "refined_grid_search_knn.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Refined best parameters:\", refined_grid_search_knn.best_params_)\n",
    "print(\"Refined best cross-validation score:\", refined_grid_search_knn.best_score_)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# %% [markdown]\n",
    "# # Hyperparameter Tuning for Machine Learning Classifiers\n",
    "# \n",
    "# **Objective**: Optimize classifier performance through systematic hyperparameter tuning.\n",
    "# \n",
    "# **Key Concepts**:\n",
    "# - **Hyperparameters**: User-defined settings controlling model behavior (e.g., tree depth, regularization strength).\n",
    "# - **Overfitting vs Underfitting**: Balance model complexity to avoid memorizing noise (overfitting) or missing patterns (underfitting).\n",
    "# - **Cross-Validation**: Robust evaluation using data subsets (critical for small planetary science datasets).\n",
    "# \n",
    "# **Tuning Methods**:\n",
    "# 1. Grid Search\n",
    "# 2. Randomized Search\n",
    "# 3. Validation Curves\n",
    "# \n",
    "# ---\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Setup\n",
    "# Add tuning-specific imports\n",
    "\n",
    "# %%\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, validation_curve\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Data Preparation\n",
    "# Use smaller subset to simulate realistic planetary data limitations\n",
    "\n",
    "# %%\n",
    "# Use 70% of original data to create \"scarce\" planetary dataset\n",
    "X_sub, _, y_sub, _ = train_test_split(\n",
    "    X, y, train_size=0.7, random_state=42, stratify=y\n",
    ")\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(\n",
    "    X_sub, y_sub, test_size=0.2, random_state=42, stratify=y_sub\n",
    ")\n",
    "\n",
    "# Scale features (only for non-tree-based models)\n",
    "X_train_sub_scaled = scaler.fit_transform(X_train_sub)\n",
    "X_test_sub_scaled = scaler.transform(X_test_sub)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Tuning Method 1: Grid Search (Logistic Regression)\n",
    "# - **Analog**: Testing all combinations of microscope settings\n",
    "# - **Parameters**: Regularization strength (`C`), penalty type (`l1/l2`)\n",
    "\n",
    "# %%\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear']  # Only solver supporting both penalties\n",
    "}\n",
    "\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(max_iter=1000),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "lr_grid.fit(X_train_sub_scaled, y_train_sub)\n",
    "\n",
    "print(f\"Best parameters: {lr_grid.best_params_}\")\n",
    "print(f\"Validation accuracy: {lr_grid.best_score_:.2f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test_sub, lr_grid.predict(X_test_sub_scaled)):.2f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Tuning Method 2: Randomized Search (k-NN)\n",
    "# - **Analog**: Randomly sampling telescope configuration parameters\n",
    "# - **Parameters**: Neighbors (`n_neighbors`), distance weighting, metric\n",
    "\n",
    "# %%\n",
    "param_dist = {\n",
    "    'n_neighbors': np.arange(3, 15),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # Manhattan (p=1) vs Euclidean (p=2)\n",
    "}\n",
    "\n",
    "knn_random = RandomizedSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_dist,\n",
    "    n_iter=20,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "knn_random.fit(X_train_sub_scaled, y_train_sub)\n",
    "\n",
    "print(f\"Best parameters: {knn_random.best_params_}\")\n",
    "print(f\"Validation accuracy: {knn_random.best_score_:.2f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test_sub, knn_random.predict(X_test_sub_scaled)):.2f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Tuning Method 3: SVM with RBF Kernel\n",
    "# - **Critical Parameters**: `C` (misclassification penalty), `gamma` (kernel width)\n",
    "# - **Geoscience Use**: Optimizing mineral boundary detection in hyperspectral data\n",
    "\n",
    "# %%\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.01, 0.1, 1]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(kernel='rbf'),\n",
    "    param_grid_svm,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "svm_grid.fit(X_train_sub_scaled, y_train_sub)\n",
    "\n",
    "print(f\"Best parameters: {svm_grid.best_params_}\")\n",
    "print(f\"Validation accuracy: {svm_grid.best_score_:.2f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test_sub, svm_grid.predict(X_test_sub_scaled)):.2f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Tuning Method 4: Random Forest\n",
    "# - **Key Parameters**: `n_estimators` (number of trees), `max_depth` (tree complexity)\n",
    "# - **Planetary Example**: Optimizing crater counting algorithms\n",
    "\n",
    "# %%\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "rf_grid.fit(X_train_sub, y_train_sub)  # No scaling for trees\n",
    "\n",
    "print(f\"Best parameters: {rf_grid.best_params_}\")\n",
    "print(f\"Validation accuracy: {rf_grid.best_score_:.2f}\")\n",
    "print(f\"Test accuracy: {accuracy_score(y_test_sub, rf_grid.predict(X_test_sub)):.2f}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Diagnostic Tool: Validation Curves\n",
    "# Visualize parameter sensitivity for SVM's `C`\n",
    "\n",
    "# %%\n",
    "C_range = np.logspace(-2, 3, 6)\n",
    "train_scores, val_scores = validation_curve(\n",
    "    SVC(gamma='auto'),\n",
    "    X_train_sub_scaled,\n",
    "    y_train_sub,\n",
    "    param_name='C',\n",
    "    param_range=C_range,\n",
    "    cv=5,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.semilogx(C_range, np.mean(train_scores, 1), label='Training')\n",
    "plt.semilogx(C_range, np.mean(val_scores, 1), label='Validation')\n",
    "plt.xlabel('C (Regularization Strength)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('SVM Complexity vs Performance')\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Tuned Model Comparison\n",
    "\n",
    "# %%\n",
    "tuned_models = {\n",
    "    'LR (Tuned)': lr_grid,\n",
    "    'k-NN (Tuned)': knn_random,\n",
    "    'SVM (Tuned)': svm_grid,\n",
    "    'RF (Tuned)': rf_grid\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in tuned_models.items():\n",
    "    if 'RF' in name:\n",
    "        pred = model.predict(X_test_sub)\n",
    "    else:\n",
    "        pred = model.predict(X_test_sub_scaled)\n",
    "    acc = accuracy_score(y_test_sub, pred)\n",
    "    results.append((name, acc))\n",
    "\n",
    "pd.DataFrame(results, columns=[\"Model\", \"Accuracy\"]).sort_values(\"Accuracy\", ascending=False)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Key Takeaways\n",
    "# - **Grid Search**: Exhaustive but computationally expensive - use for <5 parameters\n",
    "# - **Randomized Search**: Efficient for large parameter spaces - better for initial exploration\n",
    "# - **Validation Curves**: Diagnose under/overfitting by varying single parameters\n",
    "# - **Planetary Data Considerations**:\n",
    "#   - Prioritize simpler models when data is limited\n",
    "#   - Always use cross-validation with small datasets\n",
    "#   - Start with default parameters before tuning\n",
    "# \n",
    "# **Next Steps**: Feature importance analysis, automated hyperparameter optimization (Bayesian methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
